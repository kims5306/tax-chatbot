import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import os
from dotenv import load_dotenv

# Compatibility fix for Streamlit Cloud (Linux) + ChromaDB
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass


# Load params
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
CHROMA_DB_DIR = "chroma_db"
COLLECTION_NAME = "tax_laws"

# Page Config with proper title and layout
st.set_page_config(
    page_title="SeMu-Bot (Tax AI)", 
    page_icon="âš–ï¸",
    layout="wide"
)

# Custom CSS for cleaner UI
st.markdown("""
<style>
    .stChatMessage {
        background-color: #f0f2f6; 
        border-radius: 10px;
        padding: 10px;
        margin-bottom: 5px;
    }
    .user-message {
        background-color: #e8f0fe;
    }
    h1 {
        font-family: 'Helvetica', sans-serif;
        color: #333;
    }
    .stDeployButton {display:none;}
</style>
""", unsafe_allow_html=True)

# Application Title
col1, col2 = st.columns([1, 5])
with col1:
    st.image("https://cdn-icons-png.flaticon.com/512/2645/2645897.png", width=60) # Placeholder Tax Icon
with col2:
    st.title("AI Tax Accountant")
    st.caption("êµ­ì„¸ì²­ íŒë¡€, ì˜ˆê·œ ë° ì„¸ë¬´ ë²•ë ¹ ê¸°ë°˜ ì§€ëŠ¥í˜• ì±—ë´‡ (Powered by Gemini)")

# Sidebar for Settings & References
with st.sidebar:
    st.header("âš™ï¸ Settings")
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("### ğŸ“š Data Sources")
    st.caption("- main taxlaw.pdf (Internal Law)")
    st.caption("- National Law API (Precedents)")
    st.markdown("---")
    st.info("ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ:\n- ë¶€ê°€ê°€ì¹˜ì„¸ ì‹ ê³  ê¸°ê°„ì€?\n- ë²•ì¸ì„¸ ì†ê¸ˆì‚°ì… ìš”ê±´ì€?\n- ì—…ë¬´ë¬´ê´€ê°€ì§€ê¸‰ê¸ˆì´ë€?")

if not GEMINI_API_KEY:
    st.error("âŒ GEMINI_API_KEY is missing in .env")
    st.stop()

# Configure Gemini
genai.configure(api_key=GEMINI_API_KEY)

# Initialize Resources (Cached)
@st.cache_resource
def get_chroma_collection():
    client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    class LocalHuggingFaceEmbedding(chromadb.EmbeddingFunction):
        def __init__(self, model_name):
            self.model = SentenceTransformer(model_name)
        def __call__(self, input):
            return self.model.encode(input).tolist()
            
    embedding_fn = LocalHuggingFaceEmbedding(model_name)
    
    try:
        col = client.get_collection(name=COLLECTION_NAME, embedding_function=embedding_fn)
    except Exception:
        col = None
    return col

collection = get_chroma_collection()

if collection is None:
    st.warning("âš ï¸ No database found. Please run ingest.py locally first.")

# Chat Logic
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì„¸ë¬´ ë²•ë ¹ ë° íŒë¡€ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

# Display Chat History
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.chat_message("user").write(msg["content"])
    else:
        st.chat_message("assistant").write(msg["content"])

# User Input
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1. User Message
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    # 2. RAG Retrieval
    context_text = ""
    references = []
    
    if collection:
        results = collection.query(
            query_texts=[prompt],
            n_results=4  # Increased context
        )
        
        if results['documents']:
            for i, doc in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i]
                # Format context for LLM
                context_text += f"[Document {i+1}]\nTitle: {meta.get('case_name')}\nContent: {doc}\n\n"
                references.append(meta)

    # 3. Gemini Generation (Dynamic Model Selection)
    available_models = []
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
    except Exception as e:
        available_models = []

    # Prefer Flash -> Pro -> Default
    model_name = "gemini-1.5-flash" # Fallback
    for m in available_models:
        if "flash" in m:
            model_name = m
            break
        elif "pro" in m and "1.5" in m:
            model_name = m
    
    # Clean up model name (remove 'models/' prefix if present for the client, though library handles both)
    if model_name.startswith("models/"):
        model_name = model_name.replace("models/", "")
        
    model = genai.GenerativeModel(model_name)
    
    system_prompt = f"""
    ë‹¹ì‹ ì€ í•œêµ­ì˜ ìœ ëŠ¥í•œ ì„¸ë¬´ ì „ë¬¸ AI ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    
    [ë‹µë³€ ê°€ì´ë“œ]
    1. **ê·¼ê±° ì¤‘ì‹¬**: ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ë²•ë ¹ì´ë‚˜ íŒë¡€ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    2. **êµ¬ì¡°í™”**: ë‹µë³€ì€ ì½ê¸° í¸í•˜ê²Œ ë¶ˆë › í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì •ë¦¬í•˜ì„¸ìš”.
    3. **ì¶œì²˜ í‘œê¸°**: ë‹µë³€ ì¤‘ê°„ì¤‘ê°„ì— (ì°¸ê³ : ë²•ì¸ì„¸ë²• ì œXXì¡°) ì²˜ëŸ¼ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
    4. ê´€ë ¨ ìë£Œê°€ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ "ì œê³µëœ ë°ì´í„°ë² ì´ìŠ¤ ë‚´ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•˜ê³  ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë§ë¶™ì´ì„¸ìš”.
    
    [ì°¸ê³  ìë£Œ]
    {context_text}
    """
    
    full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("ë²•ë ¹ ë¶„ì„ ë° ë‹µë³€ ì‘ì„± ì¤‘..."):
            try:
                response = model.generate_content(full_prompt)
                answer = response.text
                message_placeholder.markdown(answer)
                
                # Append to history
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # Show References in Expander (Clean UI)
                if references:
                    with st.expander("ğŸ“š ì°¸ê³ í•œ ë²•ë ¹/íŒë¡€ ë¦¬ìŠ¤íŠ¸ ë³´ê¸°"):
                        for ref in references:
                            st.markdown(f"**[{ref.get('type', 'ë²•ë ¹')}] {ref.get('case_name')}**")
                            # st.caption(ref.get('filename')) # Optional
                    
            except Exception as e:
                st.error(f"Error generating response: {e}")
                
                # Debug: List available models
                try:
                    st.warning("ğŸ” Debug: Available Models for this API Key:")
                    available_models = []
                    for m in genai.list_models():
                        if 'generateContent' in m.supported_generation_methods:
                            available_models.append(m.name)
                    st.code(available_models)
                    st.info("If the list is empty, check your API Key permissions.")
                except Exception as debug_err:
                    st.error(f"Debug failed: {debug_err}")
